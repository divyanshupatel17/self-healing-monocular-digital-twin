{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "999db2d5",
            "metadata": {},
            "source": [
                "# Self-Healing Monocular Digital Twin via 3D Gaussian Splatting\n",
                "\n",
                "**Project Phase 1: Foundation & Prototype**  \n",
                "**Description:** Research-grade implementation of Monocular Visual Odometry (VO) with a Self-Healing Perception mechanism. This pipeline processes real KITTI Odometry data, reconstructs a sparse digital twin, and maintains perception continuity during sensor degradation events (e.g., blur/fog) using predictive modeling."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "57bcdf46",
            "metadata": {},
            "source": [
                "## 1. Setup & Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "724b466a",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Set Plotting Style\n",
                "plt.style.use('ggplot')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0727f17f",
            "metadata": {},
            "source": [
                "## 2. Configuration & Data Validation\n",
                "We validate the existence of the KITTI Odometry dataset paths. This notebook expects the standard Kaggle dataset structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c50bc5d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# DATASET CONFIGURATION \n",
                "KITTI_ROOT = \"/kaggle/input/kitti-odometry\" \n",
                "SEQUENCE_ID = \"00\"  # Options: 00, 01, 02...\n",
                "\n",
                "# Construct Paths\n",
                "SEQUENCES_DIR = os.path.join(KITTI_ROOT, \"sequences\")\n",
                "POSES_DIR = os.path.join(KITTI_ROOT, \"poses\")\n",
                "\n",
                "IMAGE_DIR = os.path.join(SEQUENCES_DIR, SEQUENCE_ID, \"image_2\") # Monocular Left\n",
                "CALIB_FILE = os.path.join(SEQUENCES_DIR, SEQUENCE_ID, \"calib.txt\")\n",
                "TIMES_FILE = os.path.join(SEQUENCES_DIR, SEQUENCE_ID, \"times.txt\")\n",
                "POSE_FILE = os.path.join(POSES_DIR, f\"{SEQUENCE_ID}.txt\")\n",
                "\n",
                "# Ensure all required files exist before proceeding\n",
                "if not os.path.exists(KITTI_ROOT):\n",
                "    # Fallback for local testing if not on Kaggle\n",
                "    if os.path.exists(\"./dataset\"):\n",
                "        KITTI_ROOT = \"./dataset\"\n",
                "        SEQUENCES_DIR = os.path.join(KITTI_ROOT, \"sequences\")\n",
                "        POSES_DIR = os.path.join(KITTI_ROOT, \"poses\")\n",
                "        IMAGE_DIR = os.path.join(SEQUENCES_DIR, SEQUENCE_ID, \"image_2\")\n",
                "        CALIB_FILE = os.path.join(SEQUENCES_DIR, SEQUENCE_ID, \"calib.txt\")\n",
                "        TIMES_FILE = os.path.join(SEQUENCES_DIR, SEQUENCE_ID, \"times.txt\")\n",
                "        POSE_FILE = os.path.join(POSES_DIR, f\"{SEQUENCE_ID}.txt\")\n",
                "\n",
                "assert os.path.exists(IMAGE_DIR), f\" image_2 folder not found at {IMAGE_DIR}\"\n",
                "assert os.path.exists(CALIB_FILE), f\" calib.txt missing at {CALIB_FILE}\"\n",
                "assert os.path.exists(TIMES_FILE), f\" times.txt missing at {TIMES_FILE}\"\n",
                "\n",
                "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.endswith('.png')])\n",
                "assert len(image_files) > 0, \"No images found in directory\"\n",
                "\n",
                "print(\"KITTI Sequence Loaded Successfully\")\n",
                "print(f\"   Sequence: {SEQUENCE_ID}\")\n",
                "print(f\"   Frames:   {len(image_files)}\")\n",
                "print(f\"   Path:     {IMAGE_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb555a4a",
            "metadata": {},
            "source": [
                "## 3. Data Loading & Helper Functions\n",
                "Functions to load real images, parse calibration matrices, and read ground truth poses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59c859fa",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_frame(idx):\n",
                "    \"\"\"Loads a single frame from the sequence.\"\"\"\n",
                "    img_path = os.path.join(IMAGE_DIR, image_files[idx])\n",
                "    img = cv2.imread(img_path)\n",
                "    return img # Returns BGR for OpenCV processing\n",
                "\n",
                "def load_calib(file_path):\n",
                "    \"\"\"Parses KITTI calibration file to get P0 (Left Camera Intrinsic).\"\"\"\n",
                "    with open(file_path, 'r') as f:\n",
                "        for line in f:\n",
                "            if line.startswith('P0:'):\n",
                "                vals = [float(x) for x in line.split()[1:]]\n",
                "                return np.array(vals).reshape(3, 4)[:3, :3]\n",
                "    return np.eye(3)\n",
                "\n",
                "def load_ground_truth(file_path):\n",
                "    \"\"\"Parses KITTI ground truth poses.\"\"\"\n",
                "    poses = []\n",
                "    if not os.path.exists(file_path):\n",
                "        print(\"⚠️ Ground Truth poses not found.\")\n",
                "        return []\n",
                "        \n",
                "    with open(file_path, 'r') as f:\n",
                "        for line in f:\n",
                "            # Each line is a flattened 3x4 matrix (12 floats)\n",
                "            vals = np.array([float(x) for x in line.split()])\n",
                "            T = np.eye(4)\n",
                "            T[:3, :4] = vals.reshape(3, 4)\n",
                "            poses.append(T)\n",
                "    return poses\n",
                "\n",
                "# Initialize Calibration\n",
                "K_MATRIX = load_calib(CALIB_FILE)\n",
                "print(f\"Camera Matrix (K):\\n{K_MATRIX}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e96ee922",
            "metadata": {},
            "source": [
                "## 4. Perception Modules\n",
                "\n",
                "### 4.1 Self-Healing Monitor (Blur Detection)\n",
                "Instead of random failure simulation, we implement a real quality metric. We use the **Laplacian Variance** method to detect image blur or low-texture scenarios (e.g., fog, blocked lens). If the variance drops below a threshold, the frame is degraded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "afea8e7f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def is_frame_unreliable(frame, threshold=100.0):\n",
                "    \"\"\"\n",
                "    Detects if a frame is unreliable (blurry/low texture) using Laplacian Variance.\n",
                "    Real-world application: Detects fog, rain occlusion, or camera defocus.\n",
                "    \"\"\"\n",
                "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
                "    return variance < threshold, variance"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8472711",
            "metadata": {},
            "source": [
                "### 4.2 Monocular Visual Odometry (VO)\n",
                "Standard VO pipeline: Feature Detection -> Optical Flow -> Pose Recovery."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4d3a3ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "class MonocularVO:\n",
                "    def __init__(self, K):\n",
                "        self.K = K\n",
                "        self.prev_gray = None\n",
                "        self.prev_pts = None\n",
                "        \n",
                "        self.cur_R = np.eye(3)\n",
                "        self.cur_t = np.zeros((3, 1))\n",
                "        self.trajectory = [] # Estimated path\n",
                "        \n",
                "        self.detector = cv2.FastFeatureDetector_create(threshold=20, nonmaxSuppression=True)\n",
                "\n",
                "    def process_frame(self, frame):\n",
                "        curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "        \n",
                "        # Initialization\n",
                "        if self.prev_gray is None:\n",
                "            self.prev_gray = curr_gray\n",
                "            kp = self.detector.detect(self.prev_gray, None)\n",
                "            self.prev_pts = np.array([x.pt for x in kp], dtype=np.float32)\n",
                "            return True\n",
                "\n",
                "        # Optical Flow Tracking (Lucas-Kanade)\n",
                "        curr_pts, status, err = cv2.calcOpticalFlowPyrLK(self.prev_gray, curr_gray, self.prev_pts, None)\n",
                "        \n",
                "        # Select valid points\n",
                "        status = status.reshape(-1)\n",
                "        good_prev = self.prev_pts[status == 1]\n",
                "        good_curr = curr_pts[status == 1]\n",
                "        \n",
                "        # Safety check for sparse features\n",
                "        if len(good_curr) < 5:\n",
                "            return False # Tracking Lost\n",
                "\n",
                "        # Pose Estimation (Essential Matrix)\n",
                "        E, mask = cv2.findEssentialMat(good_curr, good_prev, self.K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
                "        _, R, t, mask = cv2.recoverPose(E, good_curr, good_prev, self.K)\n",
                "        \n",
                "        # Update State (Monocular Scale Ambiguity handled by scale=1.0 for demo)\n",
                "        # In production, we would aligns this with speedometer data or ground truth\n",
                "        scale = 1.0\n",
                "        self.cur_t += scale * self.cur_R.dot(t)\n",
                "        self.cur_R = self.cur_R.dot(R)\n",
                "        \n",
                "        self.trajectory.append((self.cur_t[0][0], self.cur_t[2][0]))\n",
                "        \n",
                "        # Feature Maintenance\n",
                "        if len(good_prev) < 200:\n",
                "            kp = self.detector.detect(curr_gray, None)\n",
                "            new_pts = np.array([x.pt for x in kp], dtype=np.float32)\n",
                "            self.prev_pts = np.vstack((good_curr, new_pts))\n",
                "        else:\n",
                "            self.prev_pts = good_curr\n",
                "            \n",
                "        self.prev_gray = curr_gray\n",
                "        return True\n",
                "\n",
                "    def predict_next_state(self):\n",
                "        \"\"\"PREDICTION MODEL: Uses Constant Velocity assumption to heal gaps.\"\"\"\n",
                "        if len(self.trajectory) < 2:\n",
                "            return\n",
                "            \n",
                "        # Dead Reckoning\n",
                "        p_curr = np.array(self.trajectory[-1])\n",
                "        p_prev = np.array(self.trajectory[-2])\n",
                "        velocity = p_curr - p_prev\n",
                "        \n",
                "        new_pos = p_curr + velocity\n",
                "        \n",
                "        # Update internals without visual data\n",
                "        self.cur_t[0] = new_pos[0]\n",
                "        self.cur_t[2] = new_pos[1]\n",
                "        self.trajectory.append(tuple(new_pos))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9700ba1",
            "metadata": {},
            "outputs": [],
            "source": [
                "class GaussianDigitalTwin:\n",
                "    def __init__(self):\n",
                "        self.nodes = [] # List of (x, z) tuples representing map nodes\n",
                "\n",
                "    def update(self, position, status=\"ACTIVE\"):\n",
                "        self.nodes.append({\n",
                "            'pos': position,\n",
                "            'status': status  # 'ACTIVE' or 'HEALED'\n",
                "        })"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d40f41cd",
            "metadata": {},
            "source": [
                "## 5. Main Execution Loop\n",
                "We process the video sequence. \n",
                "- **Normal Operation:** VO updates the Digital Twin.\n",
                "- **Self-Healing:** If blur is detected (simulated via threshold or real blur), the Prediction Model takes over."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e030da6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialization\n",
                "vo = MonocularVO(K_MATRIX)\n",
                "twin = GaussianDigitalTwin()\n",
                "\n",
                "# Load Ground Truth for comparison\n",
                "gt_poses = load_ground_truth(POSE_FILE)\n",
                "\n",
                "print(\" Starting Processing Loop...\")\n",
                "for i in tqdm(range(len(image_files))):\n",
                "    frame = load_frame(i)\n",
                "    \n",
                "    # 1. Perception Health Check\n",
                "    is_bad, variance = is_frame_unreliable(frame, threshold=100)\n",
                "    \n",
                "    # Optional: Manually force failure at frame 100-150 to Demonstrate Healing\n",
                "    # (In a real scenario, this would trigger on actual fog/blur)\n",
                "    if 100 < i < 150:\n",
                "        is_bad = True\n",
                "\n",
                "    if not is_bad:\n",
                "        # HEALTHY: Run Standard VO\n",
                "        success = vo.process_frame(frame)\n",
                "        if success:\n",
                "            twin.update(vo.cur_t.flatten(), status=\"ACTIVE\")\n",
                "        else:\n",
                "            vo.predict_next_state() # Fallback if tracking fails naturally\n",
                "            twin.update(vo.cur_t.flatten(), status=\"HEALED\")\n",
                "    else:\n",
                "        # UNRELIABLE: Engage Self-Healing (Dead Reckoning)\n",
                "        vo.predict_next_state()\n",
                "        twin.update(vo.cur_t.flatten(), status=\"HEALED\")\n",
                "\n",
                "    # Limit for demo purposes\n",
                "    if i > 500: break\n",
                "\n",
                "print(\" Processing Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb3cd71b",
            "metadata": {},
            "source": [
                "## 6. Evaluation & Results\n",
                "Comparing the Reconstructed Digital Twin Trajectory (features + healed segments) against the Ground Truth."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "63c42026",
            "metadata": {},
            "outputs": [],
            "source": [
                "traj_est = np.array([n['pos'][[0, 2]] for n in twin.nodes])\n",
                "statuses = [n['status'] for n in twin.nodes]\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "\n",
                "# 1. Plot Ground Truth (if available)\n",
                "if len(gt_poses) > 0:\n",
                "    # Extract X, Z from GT poses\n",
                "    gt_path = np.array([p[:3, 3][[0, 2]] for p in gt_poses])\n",
                "    # Limit GT to frames processed\n",
                "    limit = min(len(gt_path), len(traj_est))\n",
                "    plt.plot(gt_path[:limit, 0], gt_path[:limit, 1], 'k--', label='Ground Truth', alpha=0.7)\n",
                "\n",
                "# 2. Plot Estimated Trajectory (Color-coded by Status)\n",
                "# Split into segments for coloring\n",
                "x = traj_est[:, 0]\n",
                "y = traj_est[:, 1]\n",
                "\n",
                "for i in range(len(x)-1):\n",
                "    color = 'blue' if statuses[i] == 'ACTIVE' else 'red'\n",
                "    plt.plot(x[i:i+2], y[i:i+2], color=color, linewidth=2)\n",
                "\n",
                "# Create custom legend handles\n",
                "from matplotlib.lines import Line2D\n",
                "custom_lines = [Line2D([0], [0], color='blue', lw=2), \n",
                "                Line2D([0], [0], color='red', lw=2),\n",
                "                Line2D([0], [0], color='k', lw=2, linestyle='--')]\n",
                "\n",
                "plt.legend(custom_lines, ['Perception Active (VO)', 'Self-Healing (Predicted)', 'Ground Truth'])\n",
                "\n",
                "plt.title('Digital Twin Trajectory: Active vs Healed Segments')\n",
                "plt.xlabel('X Position (m)')\n",
                "plt.ylabel('Z Position (m)')\n",
                "plt.axis('equal')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
